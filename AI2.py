from flask import Flask, request, jsonify, render_template
import os
from dotenv import load_dotenv
from huggingface_hub import InferenceClient

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

app = Flask(__name__)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ Hugging Face API í‚¤ ê°€ì ¸ì˜¤ê¸°
HF_API_KEY = os.getenv('HF_API_KEY')
if not HF_API_KEY:
    raise ValueError("Hugging Face API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Hugging Face Inference Client ì„¤ì •
client = InferenceClient(provider="sambanova", api_key=HF_API_KEY)
MODEL_NAME = "meta-llama/Llama-3.3-70B-Instruct"

# ì±—ë´‡ ì‘ë‹µ í•¨ìˆ˜
def query(message):
    try:
        messages = [{"role": "user", "content": message}]
        completion = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            max_tokens=500,
        )
        return completion.choices[0].message["content"]
    except Exception as e:
        print(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
        return "âš ï¸ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ğŸ“Œ ë©”ì¸ í˜ì´ì§€ ë Œë”ë§
@app.route('/')
def home():
    return render_template('site.html')

# ğŸ“Œ AJAX ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” API ì—”ë“œí¬ì¸íŠ¸
@app.route('/chat', methods=['POST'])
def chat():
    user_message = request.json.get("message", "")
    
    if not user_message.strip():
        return jsonify({"response": "âš ï¸ ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”!"})
    
    bot_message = query(user_message)
    return jsonify({"response": bot_message})

if __name__ == '__main__':
    app.run(debug=True, port=5000)